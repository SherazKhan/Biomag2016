{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomag 2016 - Competition 3: inter-subject exploit\n",
    "\n",
    " *Alexandre Barachant, Jean-Remi King*\n",
    "\n",
    "## Exploit description\n",
    "\n",
    "We identified that the order of the pictures was not truly randomized in the training set:\n",
    "1. The trials consisted in shuffled sequences of 2 x 6 pictures (one of each category).\n",
    "2. The overall order was identical across subjects.\n",
    "\n",
    "Consequently, we hypothesized that these structures were preserved in the test set.\n",
    "We thus assembled the predictions across subjects to increase robustness.\n",
    "\n",
    "## Approach\n",
    "\n",
    "The data was epoched from 400 ms to 1600 ms after the stimulus onset. An single estimator, adapted from our main pipeline, was fitted for each subject separately, and used to make a probabilistic estimate of the trials in the test set. \n",
    "\n",
    "Each prediction was then locally debiased and averaged across subjects.\n",
    "\n",
    "We finally check that this approach is valid by applying a cross-validation on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.estimation import HankelCovariances\n",
    "\n",
    "\n",
    "\n",
    "def epoch_data(data, window=125, offset=0):\n",
    "    \"\"\"Epoch data\"\"\"\n",
    "    MEG, trigger = data['planardat'],data['triggers']\n",
    "\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    trials = np.r_[trigger.t1, trigger.t2, trigger.t3,\n",
    "                   trigger.t4, trigger.t5, trigger.t6]\n",
    "\n",
    "    values = np.array([0]*len(trigger.t1) + [1]*len(trigger.t2) +\n",
    "                      [2]*len(trigger.t3) + [3]*len(trigger.t4) +\n",
    "                      [4]*len(trigger.t5) + [5]*len(trigger.t6))\n",
    "\n",
    "    # Epoch training set\n",
    "    ix = np.argsort(trials)\n",
    "    trials, values = trials[ix], values[ix]\n",
    "    for ii, start in enumerate(trials):\n",
    "        X.append(MEG[:, slice(start + offset, start + window + offset)])\n",
    "        y.append(values[ii])\n",
    "\n",
    "    # Epoch testing set\n",
    "    X_test = list()\n",
    "    for t in trigger.test:\n",
    "        sl = slice(t + offset, t + window + offset)\n",
    "        # Stack zeros on last trials in case window is too long\n",
    "        epoch = np.zeros((len(MEG), window))\n",
    "        epoch[:, :len(MEG[0, sl])] = MEG[:, sl]\n",
    "        X_test.append(epoch)\n",
    "\n",
    "    # Format\n",
    "    X = 1e12 * np.array(X)\n",
    "    X_test = 1e12 * np.array(X_test)\n",
    "    y = np.array(y) == 3\n",
    "\n",
    "    return X, y, X_test\n",
    "\n",
    "\n",
    "def local_debias(y_preds):\n",
    "    \"\"\"The sum of each group of 12 trials must be 1.\"\"\"\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_preds = np.reshape(y_preds, (-1, 12))\n",
    "    y_preds /= np.sum(y_preds, 1)[:, np.newaxis]\n",
    "    return y_preds.ravel()\n",
    "\n",
    "\n",
    "def local_threshold(y_preds):\n",
    "    \"\"\"Find the two maxima of each group of 12 trials\"\"\"\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_preds = y_preds.reshape(-1, 12)\n",
    "    thresholds = np.zeros_like(y_preds)\n",
    "    sorts = np.argsort(y_preds, axis=1)[:, -2:]\n",
    "    for ii, sort in enumerate(sorts):\n",
    "        thresholds[ii, sort] = 1\n",
    "    return thresholds.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here use a single estimator, because it is sufficient to obtain perfect accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = make_pipeline(\n",
    "    UnsupervisedSpatialFilter(PCA(70), average=False),\n",
    "    HankelCovariances(delays=[1, 8, 12, 64], estimator='oas'), \n",
    "    CSP(15, log=False),\n",
    "    TangentSpace('logeuclid'),\n",
    "    LogisticRegression('l2')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit on train data, aggregate test predictions across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offsets = [10, 20, 30, 40, 50]\n",
    "y_preds = np.zeros(240)\n",
    "for subject in range(1, 5):\n",
    "    data = loadmat('./data/meg_data_%da.mat' % subject, squeeze_me=True, struct_as_record=False)\n",
    "    for offset in offsets:\n",
    "        X, y, X_test = epoch_data(data, window=150, offset=offset)\n",
    "        clf.fit(X, y)\n",
    "        y_preds += local_debias(clf.predict_proba(X_test)[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our predictions in a CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# discretize probabilistic prediction\n",
    "y_test = local_threshold(y_preds)\n",
    "\n",
    "cv = StratifiedKFold(y_test, 5)\n",
    "n_test = 240\n",
    "y_preds = np.zeros((4, n_test))\n",
    "\n",
    "for subject in range(1, 5):\n",
    "    data = loadmat('./data/meg_data_%da.mat' % subject, squeeze_me=True, struct_as_record=False)\n",
    "    X, y, X_test = epoch_data(data, window=150, offset=0)\n",
    "\n",
    "    # Use CV for validation of our hypothesis\n",
    "    for train, test in cv:\n",
    "\n",
    "        # Fit on the complete train data and a subset of the test data\n",
    "        X_all = np.vstack((X, X_test[train]))\n",
    "        y_all = np.hstack((y, y_test[train]))\n",
    "        clf.fit(X_all, y_all)\n",
    "\n",
    "        # Predict on remaining test data\n",
    "        y_preds[subject-1, test] = clf.predict_proba(X_test[test])[:, -1]\n",
    "y_preds = np.mean([local_debias(y_pred) for y_pred in y_preds], axis=0)\n",
    "print(roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the same predictions for every subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = local_threshold(y_preds)\n",
    "for subject in range(1, 5):\n",
    "    results = pd.DataFrame(dict(Predictions=y_test))\n",
    "    results.to_csv('predictions_Subject%d_exploit.csv' % subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
