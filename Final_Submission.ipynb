{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomag 2016 - Competition 3\n",
    "\n",
    " *Alexandre Barachant, Jean-Remi King*\n",
    "\n",
    "## Challenge description\n",
    "\n",
    "The goal of this challenge was to discriminate the Magneto-Encephalography (MEG) responses to happy vs non-happy faces recorded from four subjects with 204 planar gradiometers. The original data were sampled at 1kHz, filtered between 0.1 and 40Hz and finally down-sampled to 125Hz.\n",
    "\n",
    "Six possible facial expressions could be presented to the subject. The subject's task was to press a button whenever a happy face was presented.\n",
    "\n",
    "Therefore, the MEG activity of interest is likely to be dominated by 1) the event-related field (ERF) specific to happy faces and 2) the neural activity induced by the following button press. \n",
    "\n",
    "The data was split into a training set and a test set. No labels were provided for the test set.\n",
    "\n",
    "In the training set, we identified that the pictures were grouped by randomly shuffled sequences of size 12, with two pictures of each category within each group. We inferred that the same regularity existed in the testing set.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Our approach consisted in ensembling three classification pipelines built to extract both evoked and induced responses. This work entirely relies on function already availables in the toolboxes [MNE-python](http://mne-tools.github.io/stable/index.html), [pyRiemann](http://pythonhosted.org/pyriemann/index.html) and [Scikit-Learn](http://scikit-learn.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from mne.decoding import UnsupervisedSpatialFilter, Vectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.estimation import ERPCovariances, XdawnCovariances, HankelCovariances\n",
    "from pyriemann.utils.base import _matrix_operator\n",
    "\n",
    "\n",
    "\n",
    "def epoch_data(data, window=125, offset=0):\n",
    "    \"\"\"Epoch data\"\"\"\n",
    "    MEG, trigger = data['planardat'], data['triggers']\n",
    "\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    trials = np.r_[trigger.t1, trigger.t2, trigger.t3,\n",
    "                   trigger.t4, trigger.t5, trigger.t6]\n",
    "\n",
    "    values = np.array([0]*len(trigger.t1) + [1]*len(trigger.t2) +\n",
    "                      [2]*len(trigger.t3) + [3]*len(trigger.t4) +\n",
    "                      [4]*len(trigger.t5) + [5]*len(trigger.t6))\n",
    "\n",
    "    # Epoch training set\n",
    "    ix = np.argsort(trials)\n",
    "    trials, values = trials[ix], values[ix]\n",
    "    for ii, start in enumerate(trials):\n",
    "        X.append(MEG[:, slice(start + offset, start + window + offset)])\n",
    "        y.append(values[ii])\n",
    "\n",
    "    # Epoch testing set\n",
    "    X_test = list()\n",
    "    for t in trigger.test:\n",
    "        sl = slice(t + offset, t + window + offset)\n",
    "        # Stack zeros on last trials in case window is too long\n",
    "        epoch = np.zeros((len(MEG), window))\n",
    "        epoch[:, :len(MEG[0, sl])] = MEG[:, sl]\n",
    "        X_test.append(epoch)\n",
    "\n",
    "    # Format\n",
    "    X = 1e12 * np.array(X)\n",
    "    X_test = 1e12 * np.array(X_test)\n",
    "    y = np.array(y) == 3\n",
    "\n",
    "    return X, y, X_test\n",
    "\n",
    "\n",
    "def local_debias(y_preds):\n",
    "    \"\"\"The sum of each group of 12 trials must be 1.\"\"\"\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_preds = np.reshape(y_preds, (-1, 12))\n",
    "    y_preds /= np.sum(y_preds, 1)[:, np.newaxis]\n",
    "    return y_preds.ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classification Pipelines\n",
    "\n",
    "The 3 pipelines rely on Riemannian geometry classifiers fitted on the covariance matrices of each trial. More specifically, these pipeline made use of the tangent space mapping described in [1, 2] followed by a logistic regression. The main difference between these pipeline resides in the definition of the features space and in the estimation of the covariance matrices. \n",
    "\n",
    "#### Covariance estimation\n",
    "Let $\\mathbf{X}_i \\in \\Re^{C \\times N}$ denote an epoch (trial) of index $i$ with $C$ the number of channels and $N$ the number of time samples. Let be $y_i$ be the class (target (happy) or non-target (not happy)) of $\\mathbf{X}_i$. \n",
    "The spatial covariance matrix $\\mathbf{\\Sigma}_i \\in \\Re^{C \\times C}$ of $\\mathbf{X}_i$ is estimated using the sample covariance matrix estimator (SCM):\n",
    "\n",
    "$$\\mathbf{\\Sigma}_i = \\frac{1}{N} \\left( \\mathbf{X}_i - \\mathbf{\\bar{x}}_i \\right) \\left( \\mathbf{X}_i - \\mathbf{\\bar{x}}_i \\right)^T$$\n",
    "\n",
    "\n",
    "where $\\mathbf{\\bar{x}}_i \\in \\Re^{C}$ is the average of the trial across time. For each of the 3 pipelines, the covariance estimation only varies in the way $\\mathbf{X}_i$ is built. Additionnaly, to ensure that the matrices are symmetric and positive definite (a requirement to use Riemannian geometry), regularization can be applied on the estimated covariances matrices. In this work, the Ledoit-wolf Shrinkage [4] or the Oracle Approximation Shrinkage [5] will be used.\n",
    "\n",
    "#### Tangent Space mapping\n",
    "Riemannian geometry provides a natural way to manipulate and measure difference between SPD matrices. In this work, we used tangent space mapping as a way to take into account the manifold structure while having a vector-representation of the matrices. For any covariance matrix $\\mathbf{\\Sigma}_i$, we define its \"tangent vector\" $\\mathbf{s}_i \\in \\Re^{C(C+1)/2}$ :\n",
    "\n",
    "$$\\mathbf{s}_i = \\mathrm{upper} \\left( \\log \\left( \\mathbf{\\Sigma}_\\mu^{-1/2} \\mathbf{\\Sigma}_i  \\mathbf{\\Sigma}_\\mu^{-1/2} \\right) \\right), $$ \n",
    "\n",
    "where $\\log$ is the matrix logarithm, $\\mathrm{upper}$ is the operator consiting in taking the upper triagular part of the matrix, applying a coefficient $\\sqrt{2}$ on its off-diagonal elements, and vectorizing the results. Finaly $\\mathbf{\\Sigma}_\\mu$ is also a covariance matrix, defining the reference point of the tangent space (its origin in the manifold). In this work, this reference point was chosen to be the log-Euclidean mean of all training data:\n",
    "\n",
    "$$\\mathbf{\\Sigma}_\\mu = \\exp \\left( \\frac{1}{N}\\sum_i^N \\log(\\mathbf{\\Sigma}_i ) \\right)$$\n",
    "\n",
    "#### Local \"debiasing\"\n",
    "\n",
    "Since the experimental design was not trully randomized, but consisted in shuffled sequences of 12 faces (with two target in each group), we applied a post-processing step on the predictions. This step consisted in ensuring that the sum of probabilities of each group of 12 trials was 1.\n",
    "\n",
    "#### Ensembling and Bagging\n",
    "\n",
    "The ensembling of the 3 pipeline is simply done by the summation of their individual predictions. To further improve the robustness of the classification, and to reduce overfitting, we also ensemble results for different set of epoching offset : 10, 20, 30, 40 and 50 time samples with a constant window of 150 time samples.\n",
    "\n",
    "We now turn to the specificities of each pipeline.\n",
    "\n",
    "### ERPCov\n",
    "\n",
    "This first pipeline is made of\n",
    "- a decomposition of the spatial filters with a Principal Component Analysis (PCA) for dimensionality reduction (70 components)\n",
    "- an estimation of the ERF Covariance to capture both evoked and induce responses\n",
    "- a Common Spatial Pattern (CSP) to reduce the dimensionality of the covariance matrices (30 components)\n",
    "- a mapping to the tangent space (with log-euclidean mean as reference point) to meaningfully vectorize the data\n",
    "- and a penalized logistic regression for the final classification stage.\n",
    "\n",
    "The classic estimation of a spatial covariance does not take into account the order of the samples. The ERPCov pipeline relies on a special form of covariance matrix estimation that embeds the temporal information about the evoked response [3]. This estimation consists in the concatenation, along the channel axis, of the averaged ERF of each class before estimating the spatial covariance matrix:\n",
    "\n",
    "$$\\mathbf{\\tilde{X}}_i = \n",
    "\\left[ \\begin{array}{c}\n",
    "\\mathbf{P}_t \\\\\n",
    "\\mathbf{P}_{nt} \\\\\n",
    "\\mathbf{X}_i\n",
    "\\end{array} \\right] $$\n",
    "\n",
    "where $\\mathbf{P}_t$ and $\\mathbf{P}_{nt}$ are the averaged (across trial) evoked response for the target and non-target class, respectively.\n",
    "\n",
    "the total dimension of the matrices is $\\tilde{C} = 3 \\times C = 3 \\times 70 = 210$. The resulting covariance matrix is thus composed by the cross-covariance of the trial with the prototypical evoked response of each class and by the standard covariance of the trial. \n",
    "\n",
    "Consequently, if the trial $\\mathbf{X}_i$ contains a target evoked response synchronized to the prototypical target response, then it will produce a specific cross-covariance structure with the target prototype. In addition, if the trial contains a task-related induced activity, the presence of the sample covariance matrice of $\\mathbf{X}_i$ will allow its detection. Therefore, this type of covariance estimation makes use of both evoked and induced responses within the same pipeline.\n",
    "\n",
    "After estimating the covariance matrices, a CSP is applied to reduce their dimension to 30 components, projected in the tangent space and classified with a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = OrderedDict()\n",
    "clfs['ERPCov'] = make_pipeline(UnsupervisedSpatialFilter(PCA(70), average=False),\n",
    "                               ERPCovariances(estimator='lwf'),\n",
    "                               CSP(30, log=False),                           \n",
    "                               TangentSpace('logeuclid'),\n",
    "                               LogisticRegression('l2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XdawnCov\n",
    "\n",
    "This pipeline is composed by \n",
    "- PCA for dimensionality reduction (50 components)\n",
    "- XdawnERP Covariance estimation\n",
    "- Tangent space mapping (with log-euclidean mean as reference point)\n",
    "- Logistic Regression for the final classification stage.\n",
    "\n",
    "The XdawnCov pipeline is similar to ERPCov, except that a Xdawn spatial filtering [6] is applied before the covariance estimation in order to increase the SNR of the ERP response.\n",
    "\n",
    "For each of the two classes (target and non-target), a set of Xdawn spatial filters are estimated and applied on the data. Let $\\mathbf{V}_{t}$ and $\\mathbf{V}_{nt}  \\in \\Re^{C \\times K}$ be the spatial filters corresponding to the class target and non target, respectively, then we get:\n",
    "\n",
    "$$\\mathbf{\\tilde{X}}_i = \n",
    "\\left[ \\begin{array}{c}\n",
    "\\mathbf{V}_{t}^T \\mathbf{P}_t \\\\\n",
    "\\mathbf{V}_{nt}^T \\mathbf{P}_{nt} \\\\\n",
    "\\mathbf{V}_{t}^T \\mathbf{X}_i\\\\\n",
    "\\mathbf{V}_{nt}^T \\mathbf{X}_i\n",
    "\\end{array} \\right] $$\n",
    "\n",
    "In this pipeline, $K=12$ Xdawn filters for each class are used, therefore the total dimention of the covariance matrices is $\\tilde{C} = 4\\times K= 48$. \n",
    "\n",
    "After estimating these covariances matrices, they are projected on the Riemannian tangent space and classified with a l2-regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs['XdawnCov'] = make_pipeline(UnsupervisedSpatialFilter(PCA(50), average=False),\n",
    "                                 XdawnCovariances(12, estimator='lwf', xdawn_estimator='lwf'),\n",
    "                                 TangentSpace('logeuclid'),\n",
    "                                 LogisticRegression('l2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HankelCov\n",
    "\n",
    "This pipeline is made of\n",
    "- PCA for dimensionality reduction (70 components),\n",
    "- Hankel Covariance estimation (delays 1, 8, 12 and 64 time samples)\n",
    "- CSP for reduction of covariance matrices (15 components)\n",
    "- Tangent space mapping (with log-euclidean mean as reference point)\n",
    "- Logistic Regression for the final classification stage.\n",
    "\n",
    "This estimation is obtained by concatenating multiple time-lagged versions of each trial. This estimation detects the auto-correlation and the cross-correlation between the channels. It is similar to the Common Spatio-Spectral Pattern algorithm. \n",
    "\n",
    "$$\\mathbf{\\tilde{X}}_i = \n",
    "\\left[ \\begin{array}{c}\n",
    "\\delta(n_1, \\mathbf{X}_i) \\\\\n",
    "\\vdots \\\\\n",
    "\\delta(n_j, \\mathbf{X}_i)\\\\\n",
    "\\vdots \\\\\n",
    "\\delta(n_J, \\mathbf{X}_i)\n",
    "\\end{array} \\right] $$\n",
    "\n",
    "where $\\delta(n_j, \\mathbf{X}_i)$ is an opperator that add a lag of $n_j$ to the data $\\mathbf{X}_i$. In the case, we chose a logaritmicaly spaced set of five lags: 0, 1, 8, 12, 64. The dimension of the covariance matrices is therefore $\\tilde{C} = 5 * C = 350$.\n",
    "A CSP was then applied to reduce the dimensionality of the covariance matrices before projecting them to the tangent space, and classifying these vectors with a penalized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs['HankelCov'] = make_pipeline(UnsupervisedSpatialFilter(PCA(70), average=False),\n",
    "                                  HankelCovariances(delays=[1, 8, 12, 64], estimator='oas'),\n",
    "                                  CSP(15, log=False),\n",
    "                                  TangentSpace('logeuclid'),\n",
    "                                  LogisticRegression('l2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation \n",
    "\n",
    "We first evaluated these classifiers on the training set using a 10-fold cross-validation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERPCov</th>\n",
       "      <th>XdawnCov</th>\n",
       "      <th>HankelCov</th>\n",
       "      <th>Ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.997125</td>\n",
       "      <td>0.99975</td>\n",
       "      <td>0.99925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909625</td>\n",
       "      <td>0.917875</td>\n",
       "      <td>0.904875</td>\n",
       "      <td>0.92925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.990875</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.979125</td>\n",
       "      <td>0.994625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.98875</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.99325</td>\n",
       "      <td>0.992875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.9720937</td>\n",
       "      <td>0.969625</td>\n",
       "      <td>0.96925</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ERPCov  XdawnCov HankelCov  Ensemble\n",
       "1      0.999125  0.997125   0.99975   0.99925\n",
       "2      0.909625  0.917875  0.904875   0.92925\n",
       "3      0.990875    0.9875  0.979125  0.994625\n",
       "4       0.98875     0.976   0.99325  0.992875\n",
       "Mean  0.9720937  0.969625   0.96925     0.979"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets = [10, 20, 30, 40, 50]\n",
    "results = pd.DataFrame(index=range(1,5), columns=clfs.keys() + ['Ensemble'])\n",
    "for subject in range(1, 5):\n",
    "    # Load the data\n",
    "    data = loadmat('./data/meg_data_%da.mat' % subject,\n",
    "                   squeeze_me=True, struct_as_record=False)\n",
    "    \n",
    "    preds = np.zeros((240, len(clfs)))\n",
    "\n",
    "    for offset in offsets:\n",
    "        # Epoching\n",
    "        X, y, _ = epoch_data(data, window=150, offset=offset)  \n",
    "\n",
    "        # define CV and prediction\n",
    "        cv = KFold(len(y), n_folds=10, shuffle=False, random_state=4343)\n",
    "\n",
    "        # iterate over models and train/test partitions\n",
    "        for jj, clf in enumerate(clfs):\n",
    "            for train, test in cv:\n",
    "                clfs[clf].fit(X[train], y[train])\n",
    "\n",
    "                # get the predictions\n",
    "                pr = clfs[clf].predict_proba(X[test])[:, -1]\n",
    "                preds[test, jj] += local_debias(pr)\n",
    "\n",
    "            results.loc[subject, clf] = roc_auc_score(y, preds[:, jj])\n",
    "    results.loc[subject, 'Ensemble'] = roc_auc_score(y, local_debias(preds.mean(1)))\n",
    "\n",
    "results.loc['Mean', :] = results.mean()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best pipeline is ERPCov with an average AUC of 0.972. The ensemble of the 3 classifiers leads to an AUC score of 0.979.\n",
    "\n",
    "## Test predictions\n",
    "\n",
    "We now apply each of the ensembling of these pipelines to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject1 Done !\n",
      "Subject2 Done !\n",
      "Subject3 Done !\n",
      "Subject4 Done !\n"
     ]
    }
   ],
   "source": [
    "offsets = [10, 20, 30, 40, 50]\n",
    "for subject in range(1, 5):\n",
    "    data = loadmat('./data/meg_data_%da.mat' % subject, squeeze_me=True, struct_as_record=False)\n",
    "    preds = np.zeros(240)\n",
    "    for offset in offsets:\n",
    "        # Epoching\n",
    "        X, y, X_test = epoch_data(data, window=150, offset=offset)  \n",
    "        for jj, clf in enumerate(clfs):\n",
    "            clfs[clf].fit(X, y)\n",
    "            preds += local_debias(clfs[clf].predict_proba(X_test)[:, -1])\n",
    "    preds = local_debias(preds)    \n",
    "    results = pd.DataFrame(data=preds, columns=['Predictions'])\n",
    "    results.to_csv('predictions_Subject%d.csv' % subject)\n",
    "    print('Subject%d Done !' % subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "The accuracy seems lowest for the second subject, but is almost perfect for the 3 other. Because the decoding of the happy face is confounded with the behavior of the subject, it is plausible that the decoder gets biased by the subject's responses, even when they are not correct.\n",
    "\n",
    "From a neuroscientific perspective, the decoding of the present dataset highlights the importance of\n",
    " - 1) truly randomizing the stimuli. Here our analyses break the independence across samples, which would make subsequent statistical estimations more complex. This is all the more true, if users start to build very opaque pipelines that may benefit from trivial inter-subject information.\n",
    " - 2) adequately decorrelating the experimental factors. Here, we cannot interpret whether the decoding is due to a sensory and/or a motor response although it is likely to be critical for the tested hypotheses.\n",
    "\n",
    "Overall, we believe these competitions offers a great way to evaluate and disseminate state-of-the-art and original methods and thank the organizers to have taken the time to make it possible. \n",
    "\n",
    "In the future, we recommend making a leader-board which would motivate the competitors in pushing their methods to their limits.\n",
    "\n",
    "\n",
    "## References \n",
    "\n",
    "> [1] A. Barachant, S. Bonnet, M. Congedo and C. Jutten, “Multiclass Brain-Computer Interface Classification by Riemannian Geometry,” in IEEE Transactions on Biomedical Engineering, vol. 59, no. 4, p. 920-928, 2012. [pdf](http://hal.archives-ouvertes.fr/docs/00/68/13/28/PDF/Barachant_tbme_final.pdf)\n",
    ">\n",
    "> [2] A. Barachant, S. Bonnet, M. Congedo and C. Jutten, “Classification of covariance matrices using a Riemannian-based kernel for BCI applications“, in NeuroComputing, vol. 112, p. 172-178, 2013. [pdf](http://hal.archives-ouvertes.fr/docs/00/82/04/75/PDF/BARACHANT_Neurocomputing_ForHal.pdf)\n",
    ">\n",
    "> [3] A. Barachant, M. Congedo ,\"A Plug&Play P300 BCI Using Information Geometry\", arXiv:1409.0107. [link](http://arxiv.org/abs/1409.0107)\n",
    ">\n",
    "> [4] O. Ledoit and M. Wolf, “A Well-Conditioned Estimator for Large-Dimensional\n",
    "Covariance Matrices”, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.\n",
    ">\n",
    "> [5] Chen et al., “Shrinkage Algorithms for MMSE Covariance Estimation”,\n",
    "IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n",
    ">\n",
    "> [6] Rivet, B., Souloumiac, A., Attina, V., & Gibert, G. (2009). xDAWN algorithm to enhance evoked potentials: application to brain–computer interface. IEEE Transactions on Biomedical Engineering, 56(8), 2035-2043."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
